1.为什么会有大数据
2005年前后，数据处理需求增长速度快于计算资源处理能力的提升速度
更加紧急有效的解决办法就是数据的并行处理
Hadoop就是利用互相联网的多台计算机使用MapReduce（一种改进的单指令多数据流[SIMD]计算技术）并行地处理计算大量数据
1.1什么是大数据
    本书，大数据定义为无法被符合服务等级协议（service level agreement, SLA）
        的单台计算机处理或（有些时候）存储的任何数据集

1.2大数据技术背后的核心思想
    核心问题：可以很快地处理数据，但从持久性的存储设备中读取的速度受到限制，这是数据处理流程的关键瓶颈
    相对于读写本地节点存储设备上的数据，通过网络传输数据会更慢
    大数据处理方法的共同特征：
        数据分布在多个节点（网络I/O速度<<本地磁盘I/O速度）
        计算程序离数据更近（集群上的节点），而不是相反
        数据的处理尽量在本地完成（网络I/O速度<<本地磁盘I/O速度）
        使用可顺序读取磁盘I/O代替随机读取磁盘I/O（数据交换速度<<数据寻道时间）
    所有大数据计算处理模式都有的目的，使输入/输出（I/O）并行化，从而提高数据处理性能
    
    1.2.1把数据分发到多个节点
        把要处理的数据分布存放在多个计算节点，不是因为数据量有数十T之巨
        最终目的是为了让大量计算节点同时参与到数据的处理计算过程中
        好处：
            每个数据块会在多个节点上有多份拷贝（Hadoop默认是一个数据块有3份拷贝），使得系统具备容错性
            为了达到数据并行处理的目的，多个节点可以同时参与数据处理过程

    1.2.2把计算逻辑移动到数据附近
        J2EE中，三/四层架构思想主导。在三/四层编程模型中，数据通过网络集中，交由应用层处理。数据分散，程序集中
        大数据系统无法处理网络过载问题。应该把数据分布存放到各个计算节点，程序也要移动到数据附近
        除了程序，程序运行所依赖的函数库也要移动到数据处理节点
        大数据系统可以集中式地部署程序代码，后台在计算任务启动之前把程序移动到各个数据处理节点

    1.2.3计算节点进行本地数据处理
        大数据系统会把计算任务尽量调度到离数据最近的节点
        某些特定的处理任务需要跨节点获取数据
        计算结果最终要汇聚到一个计算节点（MapReduce框架的Reduce阶段或其他海量数据并行化处理编程模型的类似阶段）
            起码这一步是需要跨节点获取数据的

    1.2.4优选顺序读，次之随机读
        从磁盘读取数据：
            首先，寻道（seek）：磁盘头移动到所寻找的数据所在的磁盘位置，需要花费很多时间
            之后，传输（transfer）：数据被顺序地读取出来
        读取花费时间：寻道时间 X 寻道次数 + 传输单位数据的时间 X 数据量
        顺序读寻道次数1；随机读寻道次数n（磁盘上彼此不相邻的扇区数）
        大多数数据读取密集型的大数据编程模型都利用了这个特征，数据被顺序地从磁盘上读出，然后在内存中过滤数据
        关系型数据库管理系统（RDBMS）模型往往以随机读取数据为主

    1.2.5一个例子
        一般步骤：
            1.每个计算节点读取分发给自己的全部数据，在内存中过滤，避免磁盘的寻道时间
            2.各个计算节点在处理数据时，没发现一个新的类别，就为它建立一个新的分组，把数据加到对应的分组中
            3.所有节点完成本地数据的磁盘读取工作，分别计算后，会把各自的计算结果发送到特点的计算节点
                汇聚（assembler）节点，是在计算任务伊始由所有节点协商出来的
            4.汇聚节点会汇聚全部结果，比如把各组来自不同计算节点的数据相加
            5.汇聚节点按照组把最终结果排序，输出排序结果
        该过程演示了大数据处理系统的典型特征：关注与使系统吞吐量（单位时间内系统处理的数据量）最大化
        而不是处理结果延时（请求响应的快慢，这个是评价事务性系统好坏的关键指标，因为想尽快获得响应）

1.3大数据的编程模型
    主要类型：
        大规模并行处理（Massively Parallel Processing，MPP）数据库系统：EMC的Greenplum，IBM的Netezza
        内存数据库系统：Oracle的Exalytics，SAP的HANA
        MapReduce系统：Hadoop
        整体同步并行（Bulk synchronous parallel，BSP）系统：Apache HAMA和Apache Giraph

    1.3.1大规模并行处理数据库系统
        核心思想：把数据按照某一列或某一组列的值，按照某种形式进行划分，以分别处理
        明显缺陷：在算法设计时就决定数据如何划分，划分准则通常由底层用例决定，就不适合临时的数据查询需求
        解决办法：把数据存储多份，并按照不同准则划分，根据不同的查询需求选择不同的数据集
        MPP编程模型要点：
            数据按一定的组划分，分配到不同的计算节点
            各计算节点都拥有程序所需的执行库，并对分配到该节点的数据进行数据处理
            每个计算节点读取本地数据；例外是未考虑数据分布情况就进行数据查询，计算任务会通过网络从其他节点获取数据
            每个任务都是顺序读取数据。需要的所有数据都存放在磁盘相邻位置，被一次性读取，并在内存中应用过滤条件
    
    1.3.2内存数据库系统
        内存数据库系统与MPP系统不同之处：其每个计算节点拥有巨大容量的内存，并且大部分数据会被预先加载到内存中
        本质来说，其就像是带有SQL接口的内存MPP数据库系统
        商业版本的缺点：内置了大量硬件和软件，通常费用高昂
        内存数据库系统编程模型的特征：
            数据按组划分，各节点把数据加载到内存中
            各计算节点都拥有程序所需的执行库，并对分配到该节点的数据进行数据处理
            每个计算节点读取本地数据，例外同上
            由于数据被缓存到内存，除了最初的数据加载入内存的过程外，这里不适用顺序读取数据的特性

    1.3.3MapReduce系统
        Hadoop系统对MapReduce框架的实现有如下特征：
            使用商用级别的硬件
            无需事先定义数据划分准则来把数据分配到各个计算节点
            用户仅需定义两个独立的处理过程：Map和Reduce
        Hadoop系统实现MapReduce时，数据常按照64~128MB数据块大小进行分发，每个数据块被复制两次（Hadoop系统数据备份默认参数为3）
        MapReduce编程模型的特征：
            数据以较大的数据块形式存放在HDFS（Hadoop Distributed File System， 分布式文件系统），数据库分散存储到各个节点且有冗余
            程序运行依赖库，包括Map和Reduce代码被复制发送到所有的任务节点
            每个计算节点仅读取节点本地数据，即所有节点运行Mapper，从节点本地读取数据到Mapper中
            数据以数据块的方式一次性顺序读取（大小一般为64~128MB）
        重要不足：
            不适合迭代算法，而大量数据科学计算算法要使用迭代并最终收敛于一个解
            当使用迭代算法时，MapReduce编程范型需要把每个迭代过程放到相互独立的MapReduce任务中，每次迭代的数据输出作为下次的输入
            但是MapReduce任务每次要从持久性存储中重新读取数据，所以每次迭代产生的结果需要存到持久性存储中供下次使用
            这个过程导致了不必要的I/O操作，并对系统吞吐量造成重大影响

    1.3.4整体同步并行系统
        与MapReduce过程相似，而MapReduce程序处理循环结束后即可终止，BSP系统程序执行由一系列的超步（processes，类似Map）组成
        这些超步保持栅栏同步（synchronize on a barrier；间隔同步，Synchronizing on a barrier）
        并向主节点发送数据并进行信息交换（exchange relevant information）
        每当一次迭代执行完毕，主节点会通知每个数据处理节点进行下一次迭代
        常提概念：间隔通信
            指许多线程在分别执行各自任务，这些线程在运行之前需要协商一个检查点
            处理线程到达检查点之前就要决定继续执行剩下的计算任务还是终止它们（并行的或者顺序的），以便所有线程确认何时完成数据处理任务
        允许每个类似Map的处理过程缓存上次迭代的结果，即可大幅提高整个数据处理过程的吞吐量

1.4大数据和事务性系统
    Hadoop系统使用HBase作为NoSQL数据存储，其他有Cassandra或云计算提供商（如Amazon Dynamo）的NoSQL系统
    RDBMS要求数据库必须遵守ACID准则，当数据库后台处理峰值为每秒数百万次的事物操作时，遵守该准则是巨大挑战：
        atomicity（原子性）
        consistency（一致性）
        isolation（隔离性）
        durability（持久性）
    需要对ACID准则做出妥协，理论依据是CAP理论（又称Brewer理论）：
        Consistency（一致性）：分布式系统中所有数据备份，在同一时刻有同样的值
        Availability（可用性）：在合理且明确的时间内，保证每个请求都能获得成功或者失败的结果响应
        Partition tolerance（分区容忍性）：在集群中一部分节点故障后，集群整体仍可使用
    该理论证明任何分布式系统只能同时满足其中两个特性，无法三者兼顾，对比如下：
        一致性和可用性：
            一个例子是遵守ACID准则的单机RDBMS，不满足分区容忍性，若该RDBMS当机，用户无法访问数据
        一致性和分区容忍性：
            如一个RDBMS集群。但在两阶段提交事务执行期间，系统不可用；一致性限制了系统可支持的同时执行的事务数量，也影响了系统的可用性
        可用性和分区容忍性：
            这样的系统常被归类为“最终一致性”系统。为了系统可用性和分区容忍性而牺牲系统一致性，选择这个折中使多个节点为用户显示相同的数据
            （在一个小的时间窗内，用户看到的数据可能是不同的，这个时间窗的大小取决于为用户提供服务的节点）
    如何折中选择在大数据系统设计时很关键，MapReduce只是大数据生态环境的一个组建，它常与HBase之类的产品搭配使用

1.5我们能处理多大的数据量
    前述例子中作了若干假设，比如忽略了CPU时间，同时对于大多数商用程序来说计算的复杂性不大
    但随着计算量提高，从实现的角度看任何资源消耗都要考虑
    如在数据挖掘中会用到复杂的贝叶斯统计算法，这是计算密集型应用，可增加集群节点数量来提高性能，或选其他算法替代
        类似MapReduce的大数据计算编程范型可被扩展到其他大数据计算技术中使用
        如利用计算机图形编程单元进行计算机通用计算的技术（GPGPU）可以实现计算密集型应用程序的大规模并行计算
    还忽略了网络I/O开销，所有大数据解决方案中，I/O开销是重中之重，这些开销消耗会导致串行依赖（Serial dependency）

    1.5.1一个计算密集型的列子
        假设50个计算节点处理200GB数据，平均每个计算节点本地处理4GB
        每个节点读取数据花费80秒（速率为50MB/S），无论计算多快，这些时间不能节省
        假设最终结果数据集大小为200MB，每个计算节点平均产生4MB的计算结果
        计算结果在带宽1Gbps（一个数据包为1M）的网络中传输汇聚
        传输数据需要3毫秒（网络传输1MB数据需要250微秒，每个数据包的网络传输延时为500微秒，根据早期数据）
        忽略计算耗时，整个流程不低于40.003秒
            如果集群有4000各计算节点，平均每个计算节点本地处理50MB，产生的结果数据量为0.1M
            读取50MB的数据块不会少于1秒，这意味着4000台计算节点的集群达到了数据处理速度的极限
        这个例子中同样有很多假设使系统简化，如在程序逻辑中没有串行依赖，这样的假设往往是不成立的
        一旦考虑这些开销，系统的性能表现还可能会大幅下滑

    1.5.2Amdhal定律
        Amdhal定律揭示了通过往集群增加更多计算节点的方法来提高集群性能的极限值
        简单描述：
            假如给定的解决方案，其数据计算处理并行化程度的比例达到P（P的取值范围是0到1）
            在集群计算节点数量无限的条件下（或者说集群中有海量的计算节点）
            性能提升最大为1/（1-P），如果比例达到99%，系统性能会提升100倍
        所有程序或多或少都有串行依赖，再加上磁盘I/O和网络I/O的时间消耗，无论什么样的程序计算算法，系统性能提升都无法突破这个限制

1.6大数据商用例
    大数据特征：数据量（Volume），处理速度（Velocity），数据多样性（Variety）
    ······
    开源Hadoop系统运行在多个商用服务器上，可通过添加更过计算节点来扩容集群的系统，使得ETL（加载，或ELT）性能在合理的代价上获得大幅提升
    Storm（由Twitter发起）和Apache Flume（用于分析海量日志数据信息）是专注于数据处理速度的系统，Storm处理数据的实时性会更好

1.7本章小结
    大数据渐成主流，两大推动力就是开源的Hadoop系统和元计算时代的到来，两者的发展使得以较低成本引入大规模大数据处理方法解决业务问题成为可能
    Hadoop系统是的大数据解决方案的核心，掌握之，会让开发者更加高效地使用其他编程模型
