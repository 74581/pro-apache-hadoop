1.为什么会有大数据
2005年前后，数据处理需求增长速度快于计算资源处理能力的提升速度
更加紧急有效的解决办法就是数据的并行处理
Hadoop就是利用互相联网的多台计算机使用MapReduce（一种改进的单指令多数据流[SIMD]计算技术）并行地处理计算大量数据
1.1什么是大数据
    本书，大数据定义为无法被符合服务等级协议（service level agreement, SLA）
        的单台计算机处理或（有些时候）存储的任何数据集

1.2大数据技术背后的核心思想
    核心问题：可以很快地处理数据，但从持久性的存储设备中读取的速度受到限制，这是数据处理流程的关键瓶颈
    相对于读写本地节点存储设备上的数据，通过网络传输数据会更慢
    大数据处理方法的共同特征：
        数据分布在多个节点（网络I/O速度<<本地磁盘I/O速度）
        计算程序离数据更近（集群上的节点），而不是相反
        数据的处理尽量在本地完成（网络I/O速度<<本地磁盘I/O速度）
        使用可顺序读取磁盘I/O代替随机读取磁盘I/O（数据交换速度<<数据寻道时间）
    所有大数据计算处理模式都有的目的，使输入/输出（I/O）并行化，从而提高数据处理性能
    
    1.2.1把数据分发到多个节点
        把要处理的数据分布存放在多个计算节点，不是因为数据量有数十T之巨
        最终目的是为了让大量计算节点同时参与到数据的处理计算过程中
        好处：
            每个数据块会在多个节点上有多份拷贝（Hadoop默认是一个数据块有3份拷贝），使得系统具备容错性
            为了达到数据并行处理的目的，多个节点可以同时参与数据处理过程

    1.2.2把计算逻辑移动到数据附近
        J2EE中，三/四层架构思想主导。在三/四层编程模型中，数据通过网络集中，交由应用层处理。数据分散，程序集中
        大数据系统无法处理网络过载问题。应该把数据分布存放到各个计算节点，程序也要移动到数据附近
        除了程序，程序运行所依赖的函数库也要移动到数据处理节点
        大数据系统可以集中式地部署程序代码，后台在计算任务启动之前把程序移动到各个数据处理节点

    1.2.3计算节点进行本地数据处理
        大数据系统会把计算任务尽量调度到离数据最近的节点
        某些特定的处理任务需要跨节点获取数据
        计算结果最终要汇聚到一个计算节点（MapReduce框架的Reduce阶段或其他海量数据并行化处理编程模型的类似阶段）
            起码这一步是需要跨节点获取数据的

    1.2.4优选顺序读，次之随机读
        从磁盘读取数据：
            首先，寻道（seek）：磁盘头移动到所寻找的数据所在的磁盘位置，需要花费很多时间
            之后，传输（transfer）：数据被顺序地读取出来
        读取花费时间：寻道时间 X 寻道次数 + 传输单位数据的时间 X 数据量
        顺序读寻道次数1；随机读寻道次数n（磁盘上彼此不相邻的扇区数）
        大多数数据读取密集型的大数据编程模型都利用了这个特征，数据被顺序地从磁盘上读出，然后在内存中过滤数据
        关系型数据库管理系统（RDBMS）模型往往以随机读取数据为主

    1.2.5一个例子
        一般步骤：
            1.每个计算节点读取分发给自己的全部数据，在内存中过滤，避免磁盘的寻道时间
            2.各个计算节点在处理数据时，没发现一个新的类别，就为它建立一个新的分组，把数据加到对应的分组中
            3.所有节点完成本地数据的磁盘读取工作，分别计算后，会把各自的计算结果发送到特点的计算节点
                汇聚（assembler）节点，是在计算任务伊始由所有节点协商出来的
            4.汇聚节点会汇聚全部结果，比如把各组来自不同计算节点的数据相加
            5.汇聚节点按照组把最终结果排序，输出排序结果
        该过程演示了大数据处理系统的典型特征：关注与使系统吞吐量（单位时间内系统处理的数据量）最大化
        而不是处理结果延时（请求响应的快慢，这个是评价事务性系统好坏的关键指标，因为想尽快获得响应）

1.3大数据的编程模型
    主要类型：
        大规模并行处理（Massively Parallel Processing，MPP）数据库系统：EMC的Greenplum，IBM的Netezza
        内存数据库系统：Oracle的Exalytics，SAP的HANA
        MapReduce系统：Hadoop
        整体同步并行（Bulk synchronous parallel，BSP）系统：Apache HAMA和Apache Giraph

    1.3.1大规模并行处理数据库系统
        核心思想：把数据按照某一列或某一组列的值，按照某种形式进行划分，以分别处理
        明显缺陷：在算法设计时就决定数据如何划分，划分准则通常由底层用例决定，就不适合临时的数据查询需求
        解决办法：把数据存储多份，并按照不同准则划分，根据不同的查询需求选择不同的数据集
        MPP编程模型要点：
            数据按一定的组划分，分配到不同的计算节点
            各计算节点都拥有程序所需的执行库，并对分配到该节点的数据进行数据处理
            每个计算节点读取本地数据；例外是未考虑数据分布情况就进行数据查询，计算任务会通过网络从其他节点获取数据
            每个任务都是顺序读取数据。需要的所有数据都存放在磁盘相邻位置，被一次性读取，并在内存中应用过滤条件
    
    1.3.2内存数据库系统
        内存数据库系统与MPP系统不同之处：其每个计算节点拥有巨大容量的内存，并且大部分数据会被预先加载到内存中
        本质来说，其就像是带有SQL接口的内存MPP数据库系统
        商业版本的缺点：内置了大量硬件和软件，通常费用高昂
        内存数据库系统编程模型的特征：
            数据按组划分，各节点把数据加载到内存中
            各计算节点都拥有程序所需的执行库，并对分配到该节点的数据进行数据处理
            每个计算节点读取本地数据，例外同上
            由于数据被缓存到内存，除了最初的数据加载入内存的过程外，这里不适用顺序读取数据的特性

    1.3.3MapReduce系统
        Hadoop系统对MapReduce框架的实现有如下特征：
            使用商用级别的硬件
            无需事先定义数据划分准则来把数据分配到各个计算节点
            用户仅需定义两个独立的处理过程：Map和Reduce
        Hadoop系统实现MapReduce时，数据常按照64~128MB数据块大小进行分发，每个数据块被复制两次（Hadoop系统数据备份默认参数为3）
        MapReduce编程模型的特征：
            数据以较大的数据块形式存放在HDFS（Hadoop Distributed File System， 分布式文件系统），数据库分散存储到各个节点且有冗余
            程序运行依赖库，包括Map和Reduce代码被复制发送到所有的任务节点
            每个计算节点仅读取节点本地数据，即所有节点运行Mapper，从节点本地读取数据到Mapper中
            数据以数据块的方式一次性顺序读取（大小一般为64~128MB）
        重要不足：
            不适合迭代算法，而大量数据科学计算算法要使用迭代并最终收敛于一个解
            当使用迭代算法时，MapReduce编程范型需要把每个迭代过程放到相互独立的MapReduce任务中，每次迭代的数据输出作为下次的输入
            但是MapReduce任务每次要从持久性存储中重新读取数据，所以每次迭代产生的结果需要存到持久性存储中供下次使用
            这个过程导致了不必要的I/O操作，并对系统吞吐量造成重大影响

    1.3.4整体同步并行系统
        与MapReduce过程相似，而MapReduce程序处理循环结束后即可终止，BSP系统程序执行由一系列的超步（processes，类似Map）组成
        这些超步保持栅栏同步（synchronize on a barrier；间隔同步，Synchronizing on a barrier）
        并向主节点发送数据并进行信息交换（exchange relevant information）
        每当一次迭代执行完毕，主节点会通知每个数据处理节点进行下一次迭代
        常提概念：间隔通信
            指许多线程在分别执行各自任务，这些线程在运行之前需要协商一个检查点
            处理线程到达检查点之前就要决定继续执行剩下的计算任务还是终止它们（并行的或者顺序的），以便所有线程确认何时完成数据处理任务
        允许每个类似Map的处理过程缓存上次迭代的结果，即可大幅提高整个数据处理过程的吞吐量
